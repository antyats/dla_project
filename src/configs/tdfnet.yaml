defaults:
    - model: tdfnet
    - writer: wandb
    - metrics: avss_metrics
    - datasets: full_dataset
    - dataloader: dataloader
    - transforms: spectrogram
    - _self_
optimizer:
    _target_: torch.optim.AdamW
    lr: 1e-4
    weight_decay: 1e-1
    maximize: True
lr_scheduler:
    _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
    mode: max
    factor: 0.5
    patience: 2
loss_function:
    _target_: src.loss.SI_SNR
trainer:
    log_step: 250
    n_epochs: 100
    # epoch_len: 500
    n_grad_accum_steps: 2
    device_tensors: ["mix_audio", "video", "target_audio"] # which tensors should be on device (ex. GPU)
    # resume_from: "/home/mortie/dla_project/tdfnet_from_server_fixed1.pth" # null or path to the checkpoint dir with *.pth and config.yaml
    # from_pretrained: "/home/mortie/dla_project/tdfnet_from_server_fixed2.pth"
    resume_from: "../continue learning on PC 15 (try to compile)/checkpoint-epoch4.pth"
    device: cuda # device name or "auto"
    override: False # if True, will override the previous run with the same name
    monitor: "max val_SI-SNRi" # "off" or "max/min metric_name", i.e. our goal is to maximize/minimize metric
    save_period: 5 # checkpoint each save_period epochs in addition to the best epoch
    early_stop: ${trainer.n_epochs} # epochs for early stopping
    save_dir: "saved"
    seed: 1
    amp_float_type:
        _target_: torch.__dict__.get
        _args_:
            - bfloat16
    compile_model: True
    max_grad_norm: 5
    lr_scheduler_mode: "epoch"
