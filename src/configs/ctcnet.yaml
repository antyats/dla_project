defaults:
    - model: ctcnet
    - writer: wandb
    - metrics: avss_metrics
    - datasets: full_dataset
    - dataloader: example
    - transforms: no_transforms
    - _self_
optimizer:
    _target_: torch.optim.AdamW
    lr: 1e-4
    weight_decay: 1e-1
    maximize: True
lr_scheduler:
    _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
    mode: max
    factor: 0.5
    patience: 1000
    verbose: True
loss_function:
    _target_: src.loss.SI_SNR
trainer:
    log_step: 8
    n_epochs: 100
    epoch_len: 500
    n_grad_accum_steps: 8
    device_tensors: ["mix_audio", "video", "target_audio"] # which tensors should be on device (ex. GPU)
    resume_from: null # null or path to the checkpoint dir with *.pth and config.yaml
    from_pretrained: "/mnt/d/hse/DLA/dla_project/saved/try longer training on custom loss/checkpoint-epoch5.pth"
    device: auto # device name or "auto"
    override: False # if True, will override the previous run with the same name
    monitor: "max val_SI-SNRi" # "off" or "max/min metric_name", i.e. our goal is to maximize/minimize metric
    save_period: 5 # checkpoint each save_period epochs in addition to the best epoch
    early_stop: ${trainer.n_epochs} # epochs for early stopping
    save_dir: "saved"
    seed: 1
    amp_float_type:
        _target_: torch.__dict__.get
        _args_:
            - bfloat16
    compile_model: False
    max_grad_norm: 5
